{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amontoy6/Entrega-Notebooks-Lab-2-y-3/blob/main/tp_airlines_google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ana Maria Montoya"
      ],
      "metadata": {
        "id": "VfN3zSi2zXWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQLfs9cgC6l-"
      },
      "outputs": [],
      "source": [
        "# ejemplo tomado de: \n",
        "# https://community.hortonworks.com/articles/84781/spark-text-analytics-uncovering-data-driven-topics.html\n",
        "# github: https://github.com/zaratsian/Spark/blob/master/text_analytics_datadriven_topics.json (con zeppelin)\n",
        "# otros ejemplos muy buenos: https://github.com/zaratsian/Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUVs2VCYBkVe",
        "outputId": "9c10681d-ae75-4156-fdd5-4c227be8d8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#configuración en google colab de spark y pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4woAoRwbM3C",
        "outputId": "8effdb8b-472c-45fe-9af7-b45cc72f143e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=382bcb3520558fbcfd38254c5f7672789be32c9fd0fc44ff1a8da4b7b62faf67\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kxAX41GzRh6"
      },
      "outputs": [],
      "source": [
        "#configuración en google colab\n",
        "#instalar java y spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjW7J3n9C6mH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#forma 1 de crear la sesión y el contexto Spark:\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "#forma 2 de crear la sesión y el contexto Spark:\n",
        "#sc = SparkContext.getOrCreate()\n",
        "#spark=SparkSession.builder.appName('nlp').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNHKHUNSC6mD"
      },
      "outputs": [],
      "source": [
        "#from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, isnan\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Cz07ZeJ1Fx"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.sql.types import StringType,DoubleType,IntegerType,ArrayType\n",
        "#from pyspark.sql.functions import *\n",
        "from pyspark.ml.linalg import Vectors, SparseVector\n",
        "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHO9xo0_JPpt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFHq6rlnJmed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5245de2b-85bd-47f5-c426-a007a5db9af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKxSznZPJ8iq"
      },
      "outputs": [],
      "source": [
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_nltk = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMZSzEWVC6mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a077d764-6400-4f37-d62a-676e04f234dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|uid|year_month|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|    21-Jun|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|    19-Jun|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|  2|    18-Jun|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|  3|    17-Jun|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|    17-Jun|\n",
            "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|    17-Jun|\n",
            "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|  6|    14-Jun|\n",
            "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|  7|    14-Jun|\n",
            "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|  8|    13-Jun|\n",
            "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|    13-Jun|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.csv(\"gdrive/MyDrive/Maestria/st1800-231/datasets/airlines.csv\", inferSchema=True, header=True)\n",
        "df = df.fillna({'review': ''})                               # Replace nulls with blank string\n",
        "\n",
        "# Add Unique ID\n",
        "df = df.withColumn(\"uid\", monotonically_increasing_id())     # Create Unique ID\n",
        "\n",
        "# Generate YYYY-MM variable\n",
        "df = df.withColumn(\"year_month\", df.date.substr(1,6))\n",
        "\n",
        "# Show rawdata (as DataFrame)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gn79OUsC6mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72995196-2e74-4646-fd7a-8c43aea73d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+---+----------+\n",
            "|   id|        airline|     date| location|rating|  cabin|value|recommended|              review|uid|year_month|\n",
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+---+----------+\n",
            "|10001|Delta Air Lines|21-Jun-14| Thailand|     7|Economy|    4|        YES|Flew Mar 30 NRT t...|  0|    21-Jun|\n",
            "|10002|Delta Air Lines|19-Jun-14|      USA|     0|Economy|    2|         NO|Flight 2463 leavi...|  1|    19-Jun|\n",
            "|10003|Delta Air Lines|18-Jun-14|      USA|     0|Economy|    1|         NO|Delta Website fro...|  2|    18-Jun|\n",
            "|10005|Delta Air Lines|17-Jun-14|  Ecuador|     7|Economy|    3|        YES|\"Round-trip fligh...|  4|    17-Jun|\n",
            "|10007|Delta Air Lines|14-Jun-14|       UK|     0|Economy|    1|         NO|Flight from NY La...|  6|    14-Jun|\n",
            "|10008|Delta Air Lines|14-Jun-14|      USA|     0|Economy|    1|         NO|Originally I had ...|  7|    14-Jun|\n",
            "|10010|Delta Air Lines|13-Jun-14|       UK|     9|Economy|    3|        YES|\"I flew from Heat...|  9|    13-Jun|\n",
            "|10011|Delta Air Lines|11-Jun-14|      USA|    10|Economy|    4|        YES|I was a bit stubb...| 10|    11-Jun|\n",
            "|10012|Delta Air Lines|10-Jun-14|Australia|    10|Economy|    5|        YES|JFK-LHR. Had a gr...| 11|    10-Jun|\n",
            "|10013|Delta Air Lines| 9-Jun-14|      USA|     0|Economy|    1|         NO|My wife and I fly...| 12|    9-Jun-|\n",
            "|10015|Delta Air Lines| 6-Jun-14|      USA|     0|Economy|    2|         NO|Our flight from F...| 14|    6-Jun-|\n",
            "|10016|Delta Air Lines| 5-Jun-14|      USA|     0|Economy|    1|         NO|On May 22 after a...| 15|    5-Jun-|\n",
            "|10017|Delta Air Lines| 3-Jun-14|   Canada|     9|Economy|    4|        YES|Considering how D...| 16|    3-Jun-|\n",
            "|10018|Delta Air Lines| 2-Jun-14|      USA|     9|Economy|    5|        YES|Travelled MSP-LHR...| 17|    2-Jun-|\n",
            "|10020|Delta Air Lines|28-May-14|       UK|     9|Economy|    3|        YES|Third long haul f...| 19|    28-May|\n",
            "|10021|Delta Air Lines|26-May-14|       UK|     2|Economy|    3|         NO|LHR-JFK (Virgin) ...| 20|    26-May|\n",
            "|10022|Delta Air Lines|24-May-14|      USA|     7|Economy|    3|         NO|Travelled to Lond...| 21|    24-May|\n",
            "|10023|Delta Air Lines|20-May-14|      USA|     4|Economy|    2|         NO|\"Very disappointe...| 22|    20-May|\n",
            "|10024|Delta Air Lines|20-May-14|      USA|     2|Economy|    1|         NO|\"Boy are we never...| 23|    20-May|\n",
            "|10025|Delta Air Lines|19-May-14|   Canada|     9|Economy|    5|        YES|YUL-JFK-NRT-HKG. ...| 24|    19-May|\n",
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"train_df\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM train_df where cabin='Economy'\")\n",
        "sqlDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0gEwihnC6mS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776a770e-3814-4759-891d-b0c6f22e2cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|   id|              review|\n",
            "+-----+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|\n",
            "|10002|Flight 2463 leavi...|\n",
            "|10003|Delta Website fro...|\n",
            "|10004|\"I just returned ...|\n",
            "|10005|\"Round-trip fligh...|\n",
            "|10006|Narita - Bangkok ...|\n",
            "|10007|Flight from NY La...|\n",
            "|10008|Originally I had ...|\n",
            "|10009|We flew paid busi...|\n",
            "|10010|\"I flew from Heat...|\n",
            "+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.select('id','review')\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au_sR-FTC6mW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANxqRCPtC6md"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSZsEMHfC6mZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJwZvhXqC6mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aef7737-28bc-42a5-9931-65440be14578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|   id|              review|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|\n",
            "|10003|Delta Website fro...|[delta, website, ...|\n",
            "|10004|\"I just returned ...|[returned, roundt...|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################################\n",
        "#\n",
        "#   Text Pre-processing (consider using one or all of the following):\n",
        "#       - Remove common words (with stoplist)\n",
        "#       - Handle punctuation\n",
        "#       - lowcase/upcase\n",
        "#       - Stemming\n",
        "#       - Part-of-Speech Tagging (nouns, verbs, adj, etc.)\n",
        "#\n",
        "################################################################################################\n",
        "from pyspark.sql.functions import udf,struct\n",
        "#from pyspark.sql.types import StructType\n",
        "def textprep(record):\n",
        "    text  = record[1]\n",
        "    uid   = record[0]\n",
        "    tokens = text.split()\n",
        "       \n",
        "    # Custom List of Stopwords - Add your own here\n",
        "    tokens = [re.sub('[^a-zA-Z0-9]','',word) for word in tokens]                                       # Remove special characters\n",
        "    tokens = [word.lower() for word in tokens if len(word)>2 and word.lower() not in stop_words_nltk]     # Remove stopwords and words under X length\n",
        "    return tokens\n",
        "\n",
        "udf_textprep = udf(textprep , ArrayType(StringType()))\n",
        "df = df.withColumn(\"words\", udf_textprep(struct([df[x] for x in df.columns])))\n",
        "\n",
        "#tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
        "#wordsData = tokenizer.transform(text)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI9yaPmyC6mm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9cb553-bf91-41b6-8ba9-888a320ae007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|   id|              review|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|(7306,[0,3,13,26,...|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|(7306,[0,1,6,9,16...|(7306,[0,1,6,9,16...|\n",
            "|10003|Delta Website fro...|[delta, website, ...|(7306,[0,3,4,9,17...|(7306,[0,3,4,9,17...|\n",
            "|10004|\"I just returned ...|[returned, roundt...|(7306,[0,1,2,3,5,...|(7306,[0,1,2,3,5,...|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|(7306,[0,4,9,11,1...|(7306,[0,4,9,11,1...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Term Frequency Vectorization  - Option 1 (Using hashingTF): \n",
        "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "#featurizedData = hashingTF.transform(clean_text)\n",
        "\n",
        "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
        "#cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "cvmodel = cv.fit(df)\n",
        "featurizedData = cvmodel.transform(df)\n",
        "\n",
        "vocab = cvmodel.vocabulary\n",
        "vocab_broadcast = sc.broadcast(vocab)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "df = idfModel.transform(featurizedData)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZaGGa0QC6mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab5b80f-35e3-41b2-bbab-3aa7c2cecc40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "|topic|topic_desc                                                                                 |\n",
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "|0    |[today, vegas, las, iad, delta, first, text, received, united, telling]                    |\n",
            "|1    |[nov, 2013, los, angeles, customers, tokyo, far, storage, service, dont]                   |\n",
            "|2    |[sandwich, service, served, united, salad, april, lack, broken, wheelchair, class]         |\n",
            "|3    |[hrs, philadelphia, chicago, year, philly, toronto, status, connection, check, luggage]    |\n",
            "|4    |[economy, international, extra, much, food, manchester, plenty, meals, bkk, crew]          |\n",
            "|5    |[crew, paris, one, asked, water, dallas, passengers, newark, first, milk]                  |\n",
            "|6    |[terrible, march, seats, thing, uncomfortable, lost, entertainment, charge, flights, cana] |\n",
            "|7    |[delay, hour, clt, weather, atlanta, around, sign, hotel, ticket, offer]                   |\n",
            "|8    |[seat, boarding, bags, mother, group, good, san, partner, board, offered]                  |\n",
            "|9    |[line, next, seat, available, got, hotel, left, phx, put, booked]                          |\n",
            "|10   |[would, person, southwest, delayed, issue, going, plane, extremely, tarmac, sat]           |\n",
            "|11   |[class, 1st, row, delta, seat, system, economy, excellent, comfort, american]              |\n",
            "|12   |[orlando, charlotte, cancelled, told, jacksonville, lax, credit, check, traveling, airways]|\n",
            "|13   |[voucher, dtw, dublin, bag, day, louis, soft, baggage, planes, rebook]                     |\n",
            "|14   |[good, crew, great, comfortable, seats, quick, overall, delta, cabin, excellent]           |\n",
            "|15   |[canada, airways, american, good, drink, economy, philadelphia, great, ontime, air]        |\n",
            "|16   |[fuel, tickets, get, delta, plus, give, together, needed, airport, destination]            |\n",
            "|17   |[deltas, phl, delta, business, sydney, united, lax, mexico, syd, changed]                  |\n",
            "|18   |[phoenix, gate, san, diego, united, day, agent, orleans, connection, boarding]             |\n",
            "|19   |[detroit, atl, call, horrible, boarding, got, gate, flights, went, counter]                |\n",
            "|20   |[york, new, way, oakland, wheelchair, fit, uniteds, staff, many, like]                     |\n",
            "|21   |[checkin, son, united, carry, envoy, wife, aircraft, taken, class, continental]            |\n",
            "|22   |[airport, pilot, delayed, cancelled, waiting, hours, maintenance, customer, later, minutes]|\n",
            "|23   |[denver, late, milwaukee, return, hours, connection, washington, asked, plane, without]    |\n",
            "|24   |[denver, seats, delta, kids, poor, seattle, atlanta, airplane, airlines, needs]            |\n",
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 25 Data-Driven Topics:\n",
        "lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
        "\n",
        "ldamodel = lda.fit(df)\n",
        "\n",
        "#model.isDistributed()\n",
        "#model.vocabSize()\n",
        "\n",
        "ldatopics = ldamodel.describeTopics()\n",
        "#ldatopics.show(25)\n",
        "\n",
        "def map_termID_to_Word(termIndices):\n",
        "    words = []\n",
        "    for termID in termIndices:\n",
        "        words.append(vocab_broadcast.value[termID])\n",
        "    \n",
        "    return words\n",
        "\n",
        "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
        "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
        "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PLOB_wdC6mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f2ff6a-2d75-4443-a6ee-6eec9ac07642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               words|            features|   topicDistribution|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|[0.02137091798827...|\n",
            "|[flight, 2463, le...|(7306,[0,1,6,9,16...|[0.01450588359021...|\n",
            "|[delta, website, ...|(7306,[0,3,4,9,17...|[0.01751830815799...|\n",
            "|[returned, roundt...|(7306,[0,1,2,3,5,...|[0.01077578903973...|\n",
            "|[roundtrip, fligh...|(7306,[0,4,9,11,1...|[0.00889725260905...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ldaResults = ldamodel.transform(df)\n",
        "\n",
        "ldaResults.select('words','features','topicDistribution').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhwK6qgQIB0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677882dd-3b8e-47af-8595-510fd567963d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'review', 'words', 'rawFeatures', 'features', 'topicDistribution']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "ldaResults.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h47VmTHYC6mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34b0d19-a5ab-445d-c7dd-1ae43f34aff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|MainTopic|MainTopics          |\n",
            "+---------+--------------------+\n",
            "|4        |[4, 15, 24, 14, 17] |\n",
            "|13       |[13, 20, 22, 18, 23]|\n",
            "|17       |[17, 10, 19, 9, 14] |\n",
            "|19       |[19, 14, 4, 8, 2]   |\n",
            "|7        |[7, 2, 12, 13, 9]   |\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+-----+\n",
            "|MainTopic|count|\n",
            "+---------+-----+\n",
            "|       22|   61|\n",
            "|       19|   47|\n",
            "|       11|   45|\n",
            "|        4|   44|\n",
            "|        8|   44|\n",
            "|        3|   43|\n",
            "|       13|   43|\n",
            "|        2|   42|\n",
            "|       15|   42|\n",
            "|       12|   41|\n",
            "|       17|   39|\n",
            "|        1|   39|\n",
            "|       16|   39|\n",
            "|       10|   38|\n",
            "|       23|   38|\n",
            "|       24|   38|\n",
            "|       14|   38|\n",
            "|        6|   37|\n",
            "|        7|   37|\n",
            "|        9|   35|\n",
            "+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def maintop(record):\n",
        "    vectorlist = record.tolist()\n",
        "    m = max(vectorlist)\n",
        "    maintops = [i for i, j in enumerate(vectorlist) if j == m] \n",
        "    return maintops\n",
        "\n",
        "def sorttop(record):\n",
        "    vectorlist = record.tolist()\n",
        "    unsorted = [(i,j) for i,j in enumerate(vectorlist)]\n",
        "    maintops = [i for i,j in sorted(unsorted, key=lambda tup: -tup[1])]\n",
        "    return maintops[:5]\n",
        "\n",
        "def maintop2(record):\n",
        "    return record.tolist()\n",
        "\n",
        "udf_maintop = udf(maintop, ArrayType(DoubleType()))\n",
        "udf_maintop2 = udf(maintop2, ArrayType(DoubleType()))\n",
        "udf_maintop3 = udf(sorttop, ArrayType(IntegerType()))\n",
        "\n",
        "# Extract document weights for Topics 0 and 20\n",
        "enrichedData = ldaResults.withColumn(\"MainTopics\", udf_maintop3(ldaResults.topicDistribution))\n",
        "enrichedData = enrichedData.withColumn(\"MainTopic\", enrichedData.MainTopics[0])\n",
        "\n",
        "enrichedData.select('MainTopic','MainTopics').show(5,False)\n",
        "\n",
        "enrichedData.groupBy('MainTopic').count().sort('count', ascending=False).show()\n",
        "\n",
        "#enrichedData.agg(max(\"Topic_12\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vTYmNbjC6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39634bc7-eaba-46f8-efc1-1285a59cf094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "|   id|              review|               words|         rawFeatures|            features|   topicDistribution|          MainTopics|MainTopic|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|(7306,[0,3,13,26,...|[0.02137091798827...| [4, 15, 24, 14, 17]|        4|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|(7306,[0,1,6,9,16...|(7306,[0,1,6,9,16...|[0.01450588359021...|[13, 20, 22, 18, 23]|       13|\n",
            "|10003|Delta Website fro...|[delta, website, ...|(7306,[0,3,4,9,17...|(7306,[0,3,4,9,17...|[0.01751830815799...| [17, 10, 19, 9, 14]|       17|\n",
            "|10004|\"I just returned ...|[returned, roundt...|(7306,[0,1,2,3,5,...|(7306,[0,1,2,3,5,...|[0.01077578903973...|   [19, 14, 4, 8, 2]|       19|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|(7306,[0,4,9,11,1...|(7306,[0,4,9,11,1...|[0.00889725260905...|   [7, 2, 12, 13, 9]|        7|\n",
            "|10006|Narita - Bangkok ...|[narita, bangkok,...|(7306,[0,2,3,5,10...|(7306,[0,2,3,5,10...|[0.05338746568115...|    [1, 7, 0, 8, 14]|        1|\n",
            "|10007|Flight from NY La...|[flight, guardia,...|(7306,[0,5,6,9,16...|(7306,[0,5,6,9,16...|[0.01776078240969...|[16, 17, 22, 24, 18]|       16|\n",
            "|10008|Originally I had ...|[originally, hour...|(7306,[3,4,9,16,1...|(7306,[3,4,9,16,1...|[0.08168354077482...|  [7, 0, 11, 16, 15]|        7|\n",
            "|10009|We flew paid busi...|[flew, paid, busi...|(7306,[0,1,2,4,8,...|(7306,[0,1,2,4,8,...|[0.00404786670597...|  [17, 4, 14, 11, 1]|       17|\n",
            "|10010|\"I flew from Heat...|[flew, heathrow, ...|(7306,[0,2,9,11,1...|(7306,[0,2,9,11,1...|[0.11784696089011...|    [9, 0, 4, 14, 7]|        9|\n",
            "|10011|I was a bit stubb...|[bit, stubborn, f...|(7306,[0,1,9,10,1...|(7306,[0,1,9,10,1...|[0.02649614241779...| [16, 18, 19, 5, 14]|       16|\n",
            "|10012|JFK-LHR. Had a gr...|[jfklhr, great, t...|(7306,[0,1,2,5,8,...|(7306,[0,1,2,5,8,...|[0.02514635046486...|   [22, 4, 8, 14, 1]|       22|\n",
            "|10013|My wife and I fly...|[wife, fly, frequ...|(7306,[0,5,9,12,1...|(7306,[0,5,9,12,1...|[0.03043170940078...| [24, 3, 23, 22, 16]|       24|\n",
            "|10014|DL 1134 PBI-ATL. ...|[1134, pbiatl, gr...|(7306,[0,1,3,4,9,...|(7306,[0,1,3,4,9,...|[0.01484825997273...|   [6, 5, 7, 14, 16]|        6|\n",
            "|10015|Our flight from F...|[flight, fairbank...|(7306,[0,2,3,9,16...|(7306,[0,2,3,9,16...|[0.02984988996360...|  [6, 21, 0, 11, 18]|        6|\n",
            "|10016|On May 22 after a...|[may, 6hour, dela...|(7306,[0,1,9,12,2...|(7306,[0,1,9,12,2...|[0.01051941231146...| [22, 12, 10, 24, 4]|       22|\n",
            "|10017|Considering how D...|[considering, del...|(7306,[0,2,3,5,9,...|(7306,[0,2,3,5,9,...|[0.02177616318293...|  [7, 15, 16, 14, 6]|        7|\n",
            "|10018|Travelled MSP-LHR...|[travelled, msplh...|(7306,[0,1,3,8,9,...|(7306,[0,1,3,8,9,...|[0.04290867324977...| [17, 14, 11, 0, 23]|       17|\n",
            "|10019|JFK-LAX on a 757-...|[jfklax, 757200, ...|(7306,[0,2,8,10,1...|(7306,[0,2,8,10,1...|[0.01286590175018...|  [1, 11, 14, 4, 15]|        1|\n",
            "|10020|Third long haul f...|[third, long, hau...|(7306,[0,1,3,4,8,...|(7306,[0,1,3,4,8,...|[0.02208850032342...|  [15, 14, 9, 6, 19]|       15|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "enrichedData.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}