{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amontoy6/Entrega-Notebooks-Lab-2-y-3/blob/main/lab7_NLP_PySpark_bow_tf_idf_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Ana Maria Montoya Mesa\n",
        "# Universidad EAFIT \n",
        "# 2023-1\n",
        "#"
      ],
      "metadata": {
        "id": "vU3BljHM4U3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhV_4DheLYkI",
        "outputId": "1dec02d9-775d-42dd-fa77-56ee50944869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#configuraci칩n en google colab de spark y pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUnhewReLYkL"
      },
      "outputs": [],
      "source": [
        "#instalar java y spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECTrjkL5LYkL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#forma 1 de crear la sesi칩n y el contexto Spark:\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "#forma 2 de crear la sesi칩n y el contexto Spark:\n",
        "#sc = SparkContext.getOrCreate()\n",
        "#spark=SparkSession.builder.appName('nlp').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HEASDpkPLYkM"
      },
      "outputs": [],
      "source": [
        "#myrdd = sc.wholeTextFiles('../datasets/papers_sample_pdf/*.txt')\n",
        "#df = myrdd.toDF(schema=['filename','content'])\n",
        "#df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq4f5laJLYkM"
      },
      "outputs": [],
      "source": [
        "df=spark.createDataFrame([(1,'I really liked this movie'),\n",
        "                         (2,'I would recommend this movie to my friends'),\n",
        "                         (3,'movie was alright but acting was horrible'),\n",
        "                         (4,'I am never watching that movie ever again')],\n",
        "                        ['user_id','content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_viXL0iLYkN",
        "outputId": "8bc010c4-5d7e-4832-afa5-c1ffe2b52f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- content: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j6k1--wLYkN",
        "outputId": "bdcae487-9ffd-45c3-dee8-c637104751af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- content: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------+--------------------+--------------------+\n",
            "|user_id|             content|              tokens|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|I really liked th...|[i, really, liked...|\n",
            "|      2|I would recommend...|[i, would, recomm...|\n",
            "|      3|movie was alright...|[movie, was, alri...|\n",
            "|      4|I am never watchi...|[i, am, never, wa...|\n",
            "+-------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "tokenization=Tokenizer(inputCol='content',outputCol='tokens')\n",
        "tokenized_df=tokenization.transform(df)\n",
        "tokenized_df.printSchema()\n",
        "tokenized_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvDPBLFwLYkN",
        "outputId": "24f7f330-d0d7-4c07-f16a-f37f4341fdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+----------------------------------+\n",
            "|tokens                                             |refined_tokens                    |\n",
            "+---------------------------------------------------+----------------------------------+\n",
            "|[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
            "|[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
            "|[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
            "|[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
            "+---------------------------------------------------+----------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# stopwords removal \n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
        "refined_df=stopword_removal.transform(tokenized_df)\n",
        "refined_df.select(['tokens','refined_tokens']).show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ-MmPfjLYkO",
        "outputId": "a83c5cd0-30cf-4e36-9035-d44ef347df3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user_id', 'content', 'tokens', 'refined_tokens']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "refined_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pUhDDy7LYkO"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqUDY3EMLYkO"
      },
      "outputs": [],
      "source": [
        "len_udf = udf(lambda s: len(s), IntegerType())\n",
        "\n",
        "refined_count_df = refined_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klzoyVb8LYkP"
      },
      "outputs": [],
      "source": [
        "refined_count_df.orderBy(rand()).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmYAvqYpLYkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a83288-a7eb-4c5a-a7db-b69e990930cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+---------------------------------+\n",
            "|refined_tokens                    |features                         |\n",
            "+----------------------------------+---------------------------------+\n",
            "|[really, liked, movie]            |(11,[0,2,3],[1.0,1.0,1.0])       |\n",
            "|[recommend, movie, friends]       |(11,[0,6,7],[1.0,1.0,1.0])       |\n",
            "|[movie, alright, acting, horrible]|(11,[0,1,5,10],[1.0,1.0,1.0,1.0])|\n",
            "|[never, watching, movie, ever]    |(11,[0,4,8,9],[1.0,1.0,1.0,1.0]) |\n",
            "+----------------------------------+---------------------------------+\n",
            "\n",
            "['movie', 'horrible', 'liked', 'really', 'watching', 'alright', 'friends', 'recommend', 'ever', 'never', 'acting']\n"
          ]
        }
      ],
      "source": [
        "# Count Vectorizer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
        "cv_df=count_vec.fit(refined_df).transform(refined_df)\n",
        "cv_df.select(['refined_tokens','features']).show(4,False)\n",
        "bow = count_vec.fit(refined_df).vocabulary\n",
        "print(bow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKUUTAwxLYkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c47c731-7805-4fbd-9722-04548564ea70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+--------------------+--------------------+\n",
            "|user_id|             content|              tokens|      refined_tokens|         tf_features|\n",
            "+-------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      1|I really liked th...|[i, really, liked...|[really, liked, m...|(11,[9,10],[2.0,1...|\n",
            "|      2|I would recommend...|[i, would, recomm...|[recommend, movie...|(11,[1,6,9],[1.0,...|\n",
            "|      3|movie was alright...|[movie, was, alri...|[movie, alright, ...|(11,[1,6,9,10],[1...|\n",
            "|      4|I am never watchi...|[i, am, never, wa...|[never, watching,...|(11,[0,7,8,9],[1....|\n",
            "+-------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TF with HashingTF\n",
        "from pyspark.ml.feature import HashingTF\n",
        "# podria utilizar numFeatures como el tama침o del Bag of Words:\n",
        "l = len(bow)\n",
        "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=l)\n",
        "#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=11)\n",
        "# compare la salida e interprete con y sin numFeatures:\n",
        "#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')\n",
        "\n",
        "hashing_df=hashing_vec.transform(refined_df)\n",
        "hashing_df.show(4)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}